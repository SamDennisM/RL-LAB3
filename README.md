# ğŸ§­ Markov Decision Process (MDP) â€” Value Iteration & Simulation  
> **Gridworld + Reinforcement Learning + Policy Visualization + Simulation Analytics**

<p align="center">
  <img src="https://img.shields.io/badge/Algorithm-Value%20Iteration-1E90FF?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Framework-Matplotlib-FF5733?logo=plotly&logoColor=white" />
  <img src="https://img.shields.io/badge/Category-Reinforcement%20Learning-8A2BE2" />
  <img src="https://img.shields.io/badge/Simulation-Agent%20Policy-green" />
  <img src="https://img.shields.io/badge/Visualization-Seaborn-yellow" />
</p>

---

## âœ¨ Overview

This project demonstrates a complete **Markov Decision Process (MDP)** simulation using **Value Iteration** to find optimal policies in a **stochastic Gridworld environment**.  
It provides deep insights into **decision-making under uncertainty** and showcases **beautiful visualizations** for learning and analysis.  

ğŸ’¡ **Goal:** Train an agent to navigate a grid with rewards, penalties, and obstacles while maximizing cumulative reward through Value Iteration.

---

## ğŸš€ Features

- ğŸ§® **Full MDP Implementation**
  - States, actions, rewards, transitions, and terminal states  
- âš™ï¸ **Value Iteration Algorithm**
  - Computes optimal value functions with convergence visualization  
- ğŸ§­ **Policy Extraction**
  - Generates and visualizes the best possible moves (arrows)  
- ğŸ” **Agent Simulation**
  - Runs both greedy and Îµ-greedy trajectories across the grid  
- ğŸ“Š **Visualization Suite**
  - Heatmaps, reward plots, policy maps, and convergence curves  
- ğŸ“ˆ **Dynamic Comparative Analysis**
  - Compare learning across different discount factors (Î³)  

---

## ğŸ§  Algorithmic Insight

**Value Iteration** iteratively refines the state-value function until convergence, following the **Bellman Optimality Equation**:

\[
V_{k+1}(s) = \max_a \sum_{s'} P(s'|s, a) [R(s,a,s') + \gamma V_k(s')]
\]

After convergence, the **optimal policy** is derived as:

\[
\pi^*(s) = \arg\max_a \sum_{s'} P(s'|s, a) [R(s,a,s') + \gamma V^*(s')]
\]

---

## ğŸ“ˆ Visualizations

ğŸ¨ The notebook generates rich, intuitive plots for understanding agent behavior:

- ğŸ—ºï¸ **Value Function Heatmap** â€“ visualizes learned state values  
- ğŸ§­ **Optimal Policy Map** â€“ arrows show the best move per cell  
- ğŸ“‰ **Convergence Graph** â€“ shows max Î” per iteration (log scale)  
- ğŸ”¥ **State Visit Heatmap** â€“ highlights frequently visited areas  
- ğŸ§© **Trajectory Path Visualization** â€“ step-by-step navigation  
- ğŸ“Š **Reward Progression Plot** â€“ running average of episodic rewards  

---

## ğŸ’» Tech Stack

<p align="center">
  <img src="https://img.shields.io/badge/Language-Python-3776AB?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Visualization-Matplotlib%20%7C%20Seaborn-FFDD00?logo=plotly&logoColor=white" />
  <img src="https://img.shields.io/badge/Data-Numpy%20%7C%20Pandas-blue" />
  <img src="https://img.shields.io/badge/Progress-tqdm-00C853" />
</p>

---

## ğŸ§© How It Works

1ï¸âƒ£ Define the **Gridworld MDP** (states, actions, transitions, rewards).  
2ï¸âƒ£ Apply **Value Iteration** until convergence.  
3ï¸âƒ£ Extract the **Optimal Policy** from the value function.  
4ï¸âƒ£ Simulate multiple **agent trajectories** (greedy and Îµ-greedy).  
5ï¸âƒ£ Generate **visual reports** and performance insights.  

---

## ğŸ§  Key Learnings

- **Discount Factor (Î³)** controls long-term vs. short-term rewards.  
- **Exploration (Îµ-greedy)** ensures non-deterministic exploration.  
- **Convergence rate** depends on grid design and Î³ value.  
- **Visualization** is crucial for interpreting policy efficiency.  

---

## ğŸ§ª Future Enhancements

- ğŸ”„ Implement **Policy Iteration**, **Q-Learning**, and **SARSA**  
- ğŸ§© Extend environment to **continuous state space**  
- ğŸ§± Add **OpenAI Gym compatibility**  
- ğŸï¸ Create **animated trajectories (GIF/MP4)**  
- ğŸ§¾ Export **automated analytical reports (PDF)**  

---

## ğŸ‘¨â€ğŸ’» Author

**Sam Dennis**  
ğŸ“ MSc AI & ML â€” Christ University  
ğŸ’¼ Research Focus: Reinforcement Learning & IoT Security  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/samdennis) â€¢ [GitHub](https://github.com/your-username)

---

> â€œLearning is not about immediate reward, but about understanding the long game.â€ ğŸ§ 
